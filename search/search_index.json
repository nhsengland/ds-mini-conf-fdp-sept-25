{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FDP - A Simple Transforms Pipeline \u00b6 You can use the instructions below to follow along with the workshop at your own pace! UI - it's all files! \u00b6 The UI in FDP is file-based (or seems to be) When you\u2019re working in FDP all your stuff will be in a folder for your project e.g.: /super_important_analysis /data code models exploration memes etc. Mission 1 - Make folders \u00b6 Go to the Developer Sandbox (or any other place you have write access in FDP): Developer Sandbox In here you can go into the \"[Learn] Palantir Training\" Project and make a folder with your name Create a folder structure like this: /Tony_Stark /super_important_analyis /data /code It probably makes sense to replace \u201cTony_Stark\u201d with your own name. Datasets \u00b6 A Dataset (capital \u201cD\u201d, it\u2019s a thing) in Foundry is appropriately named because this is where data goes. A Dataset also manages access control, the schema, version control, metadata etc. A Dataset can contain tabular or unstructured data (i.e. files) There is a Python API we can use read and write to Datasets, and convert to Pandas/Spark DFs Mission 2 \u2013 Make a Dataset \u00b6 In your /data folder: Click \u201cNew\u201d Search for \u201cDataset\u201d Click \u201cDataset\u201d Call it anything you want Mission 3 \u2013 Import some data \u00b6 Download this sample of artificial HES data: Artificial HES APC 2122 Back in FDP, go to the Dataset you created and then: Click \"Import\" Add a justifiction (e.g., \"artificial data\") Drag your data into the \"Drop files here\" box Where to write code \u00b6 Code Repositories Browser based IDE Lots of Git function accessible here (e.g., merging PRs) Has Spark / parallel processing No command line Code Workspaces Analysis focus \u2013 Jupyter notebooks Is backed by a Code Repositories! (but notebooks won\u2019t run here) No parallel processing Has command line Mission 4 \u2013 Create a Code Repository \u00b6 Go to your \u201ccode\u201d folder Click \u201cNew\u201d Search for \u201ccode repository\u201d Click \u201cCode Repository\u201d Mission 5 \u2013 Set up Transforms \u00b6 When you create a code repository, you get different options These determine the starting code/boilerplate you get Steps: Choose \u201cPipelines\u201d Choose \u201cPython\u201d Give the Code Repo a name Choose \u201cDistributed transform (Spark)\u201d Click \"Add\" next to the Input Dataset, and choose the one you created earlier Clike \"Add\" next to the Output Dataset, and choose your \"data\" folder Code Repositories \u00b6 The Code Repositories UI gives you: Git functionality PRs, code review, branching etc. Protect branches Install dependencies Checks Debugger Preview - like display(df) Build \u2013 like df.write Scheduling (time, trigger) You get a lot of hidden files too that are used behind the scenes to make your Code Repository work! These get updated automatically by Foundry - just let it do its thing. Create a branch using the git icon towards the top left of the screen. Transforms \u00b6 Transforms are like the steps or building blocks of a pipeline Each one typically outputs a new Dataset. So you get a chain of transforms where the input to the next one is the output from the previous one. For hefty data processing, the Transforms API in Code Repositories is the way to go (to use Spark) They live in the \"datasets\" folder by default. You could have a file for each transform in here. Transforms - anatomy \u00b6 There are different types of Transforms. But they have three things in common: Imports - you import the relevant packages Decorator - to define the input and output datasets Compute function - where your data processing actually happens Mission 6 - Create our Transform \u00b6 Use the code below in your spark-transforms.py file. # These imports should have been auto generated for you from transforms.api import transform , TransformContext , Input , TransformInput , Output , TransformOutput # as should the paths to your input and output Datasets (it should have created your output Dataset based on the path and name you gave earlier) @transform ( hes = Input ( \"\" ), hes_output = Output ( \"\" ) ) def compute ( ctx : TransformContext , hes : TransformInput , hes_output : TransformOutput ): # Convert the hes TransformInput instance into a PySpark DataFrame df_input = hes . dataframe () # Our PySpark code goes here df_output = df_input . limit ( 1 ) # Pass the result of our DataProcessing to the write_dataframe() method of our TransformOutput instance hes_output . write_dataframe ( df_output ) Transforms - Running them \u00b6 First you'll want to use the \"Preview\" button on the menu bar near the top Through the menu at the bottom you can see checks on your code, test outputs, and outputs from previews and builds If it looks OK, then you can click \"Build\" at the top to actually save the output data to the Dataset Seeing the results \u00b6 Now go to the Dataset You should see your data here But note that your data branches too!","title":"Home"},{"location":"#fdp-a-simple-transforms-pipeline","text":"You can use the instructions below to follow along with the workshop at your own pace!","title":"FDP - A Simple Transforms Pipeline"},{"location":"#ui-its-all-files","text":"The UI in FDP is file-based (or seems to be) When you\u2019re working in FDP all your stuff will be in a folder for your project e.g.: /super_important_analysis /data code models exploration memes etc.","title":"UI - it's all files!"},{"location":"#mission-1-make-folders","text":"Go to the Developer Sandbox (or any other place you have write access in FDP): Developer Sandbox In here you can go into the \"[Learn] Palantir Training\" Project and make a folder with your name Create a folder structure like this: /Tony_Stark /super_important_analyis /data /code It probably makes sense to replace \u201cTony_Stark\u201d with your own name.","title":"Mission 1 - Make folders"},{"location":"#datasets","text":"A Dataset (capital \u201cD\u201d, it\u2019s a thing) in Foundry is appropriately named because this is where data goes. A Dataset also manages access control, the schema, version control, metadata etc. A Dataset can contain tabular or unstructured data (i.e. files) There is a Python API we can use read and write to Datasets, and convert to Pandas/Spark DFs","title":"Datasets"},{"location":"#mission-2-make-a-dataset","text":"In your /data folder: Click \u201cNew\u201d Search for \u201cDataset\u201d Click \u201cDataset\u201d Call it anything you want","title":"Mission 2 \u2013 Make a Dataset"},{"location":"#mission-3-import-some-data","text":"Download this sample of artificial HES data: Artificial HES APC 2122 Back in FDP, go to the Dataset you created and then: Click \"Import\" Add a justifiction (e.g., \"artificial data\") Drag your data into the \"Drop files here\" box","title":"Mission 3 \u2013 Import some data"},{"location":"#where-to-write-code","text":"Code Repositories Browser based IDE Lots of Git function accessible here (e.g., merging PRs) Has Spark / parallel processing No command line Code Workspaces Analysis focus \u2013 Jupyter notebooks Is backed by a Code Repositories! (but notebooks won\u2019t run here) No parallel processing Has command line","title":"Where to write code"},{"location":"#mission-4-create-a-code-repository","text":"Go to your \u201ccode\u201d folder Click \u201cNew\u201d Search for \u201ccode repository\u201d Click \u201cCode Repository\u201d","title":"Mission 4 \u2013 Create a Code Repository"},{"location":"#mission-5-set-up-transforms","text":"When you create a code repository, you get different options These determine the starting code/boilerplate you get Steps: Choose \u201cPipelines\u201d Choose \u201cPython\u201d Give the Code Repo a name Choose \u201cDistributed transform (Spark)\u201d Click \"Add\" next to the Input Dataset, and choose the one you created earlier Clike \"Add\" next to the Output Dataset, and choose your \"data\" folder","title":"Mission 5 \u2013 Set up Transforms"},{"location":"#code-repositories","text":"The Code Repositories UI gives you: Git functionality PRs, code review, branching etc. Protect branches Install dependencies Checks Debugger Preview - like display(df) Build \u2013 like df.write Scheduling (time, trigger) You get a lot of hidden files too that are used behind the scenes to make your Code Repository work! These get updated automatically by Foundry - just let it do its thing. Create a branch using the git icon towards the top left of the screen.","title":"Code Repositories"},{"location":"#transforms","text":"Transforms are like the steps or building blocks of a pipeline Each one typically outputs a new Dataset. So you get a chain of transforms where the input to the next one is the output from the previous one. For hefty data processing, the Transforms API in Code Repositories is the way to go (to use Spark) They live in the \"datasets\" folder by default. You could have a file for each transform in here.","title":"Transforms"},{"location":"#transforms-anatomy","text":"There are different types of Transforms. But they have three things in common: Imports - you import the relevant packages Decorator - to define the input and output datasets Compute function - where your data processing actually happens","title":"Transforms - anatomy"},{"location":"#mission-6-create-our-transform","text":"Use the code below in your spark-transforms.py file. # These imports should have been auto generated for you from transforms.api import transform , TransformContext , Input , TransformInput , Output , TransformOutput # as should the paths to your input and output Datasets (it should have created your output Dataset based on the path and name you gave earlier) @transform ( hes = Input ( \"\" ), hes_output = Output ( \"\" ) ) def compute ( ctx : TransformContext , hes : TransformInput , hes_output : TransformOutput ): # Convert the hes TransformInput instance into a PySpark DataFrame df_input = hes . dataframe () # Our PySpark code goes here df_output = df_input . limit ( 1 ) # Pass the result of our DataProcessing to the write_dataframe() method of our TransformOutput instance hes_output . write_dataframe ( df_output )","title":"Mission 6 - Create our Transform"},{"location":"#transforms-running-them","text":"First you'll want to use the \"Preview\" button on the menu bar near the top Through the menu at the bottom you can see checks on your code, test outputs, and outputs from previews and builds If it looks OK, then you can click \"Build\" at the top to actually save the output data to the Dataset","title":"Transforms - Running them"},{"location":"#seeing-the-results","text":"Now go to the Dataset You should see your data here But note that your data branches too!","title":"Seeing the results"}]}