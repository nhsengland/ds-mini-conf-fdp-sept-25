{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Science Sept 2025 Mini Conference","text":""},{"location":"#fdp-a-simple-transforms-pipeline","title":"FDP - A Simple Transforms Pipeline","text":"<p>You can use the instructions below to follow along with the workshop at your own pace!</p>"},{"location":"#ui-its-all-files","title":"UI - it's all files!","text":"<p>The UI in FDP is file-based (or seems to be)</p> <p>When you\u2019re working in FDP all your stuff will be in a folder for your project e.g.:</p> <ul> <li>/super_important_analysis</li> <li>/data</li> <li>code</li> <li>models</li> <li>exploration</li> <li>memes</li> <li>etc.</li> </ul>"},{"location":"#mission-1-make-folders","title":"Mission 1 - Make folders","text":"<p>Go to the Developer Sandbox, Solex, or any place you have write access to in FDP.</p> <p>Create a folder structure like this:</p> <ul> <li>/Tony_Stark</li> <li>/super_important_analyis</li> <li>/data</li> <li>/code</li> </ul> <p>It probably makes sense to replace \u201cTony_Stark\u201d with your own name.</p>"},{"location":"#datasets","title":"Datasets","text":"<p>A Dataset (capital \u201cD\u201d, it\u2019s a thing) in Foundry is appropriately named because this is where data goes. </p> <p>A Dataset also manages access control, the schema, version control, metadata etc.</p> <p>A Dataset can contain tabular or unstructured data (i.e. files)</p> <p>There is a Python API we can use read and write to Datasets, and convert to Pandas/Spark DFs</p>"},{"location":"#mission-2-make-a-dataset","title":"Mission 2 \u2013 Make a Dataset","text":"<p>In your /data folder:</p> <ol> <li>Click \u201cNew\u201d</li> <li>Search for \u201cDataset\u201d</li> <li>Click \u201cDataset\u201d</li> <li>Call it anything you want</li> </ol>"},{"location":"#mission-3-import-some-data","title":"Mission 3 \u2013 Import some data","text":"<p>Download this sample of artificial HES data:</p> <p>Artificial HES APC 2122</p> <p>Back in FDP, go to the Dataset you created and then: </p> <ul> <li>Click \"Import\"</li> <li>Add a justifiction (e.g., \"artificial data\")</li> <li>Drag your data into the \"Drop files here\" box</li> </ul>"},{"location":"#where-to-write-code","title":"Where to write code","text":"<ul> <li>Code Repositories</li> <li>Browser based IDE</li> <li>Lots of Git function accessible here (e.g., merging PRs)</li> <li>Has Spark / parallel processing</li> <li> <p>No command line</p> </li> <li> <p>Code Workspaces</p> </li> <li>Analysis focus \u2013 Jupyter notebooks</li> <li>Is backed by a Code Repositories! (but notebooks won\u2019t run here)</li> <li>No parallel processing</li> <li>Has command line</li> </ul>"},{"location":"#mission-4-create-a-code-repository","title":"Mission 4 \u2013 Create a Code Repository","text":"<ul> <li>Go to your \u201ccode\u201d folder</li> <li>Click \u201cNew\u201d</li> <li>Search for \u201ccode repository\u201d</li> <li>Click \u201cCode Repository\u201d</li> </ul>"},{"location":"#mission-5-set-up-transforms","title":"Mission 5 \u2013 Set up Transforms","text":"<p>When you create a code repository, you get different options</p> <p>These determine the starting code/boilerplate you get</p> <p>Steps:</p> <ol> <li>Choose \u201cPipelines\u201d</li> <li>Choose \u201cPython\u201d</li> <li>Give the Code Repo a name</li> <li>Choose \u201cDistributed transform (Spark)\u201d</li> <li>Click \"Add\" next to the Input Dataset, and choose the one you created earlier</li> <li>Clike \"Add\" next to the Output Dataset, and choose your \"data\" folder</li> </ol>"},{"location":"#code-repositories","title":"Code Repositories","text":"<p>The Code Repositories UI gives you:</p> <ul> <li>Git functionality</li> <li>PRs, code review, branching etc.<ul> <li>Protect branches</li> <li>Install dependencies</li> </ul> </li> <li>Checks</li> <li>Debugger</li> <li>Preview - like display(df)</li> <li>Build \u2013 like df.write</li> <li>Scheduling (time, trigger)</li> </ul> <p>You get a lot of hidden files too that are used behind the scenes to make your Code Repository work!  These get updated automatically by Foundry - just let it do its thing.</p> <p>Create a branch using the git icon towards the top left of the screen.</p>"},{"location":"#transforms","title":"Transforms","text":"<p>Transforms are like the steps or building blocks of a pipeline</p> <p>Each one typically outputs a new Dataset. So you get a chain of transforms where the input to the next one is the output from the previous one.</p> <p>For hefty data processing, the Transforms API in Code Repositories is the way to go (to use Spark)</p> <p>They live in the \"datasets\" folder by default. You could have a file for each transform in here.</p>"},{"location":"#transforms-anatomy","title":"Transforms - anatomy","text":"<p>There are different types of Transforms. But they have three things in common:</p> <ul> <li> <p>Imports - you import the relevant packages</p> </li> <li> <p>Decorator - to define the input and output datasets</p> </li> <li> <p>Compute function - where your data processing actually happens</p> </li> </ul>"},{"location":"#mission-6-create-our-transform","title":"Mission 6 - Create our Transform","text":"<p>Use the code below in your spark-transforms.py file.</p> <pre><code># These imports should have been auto generated for you\nfrom transforms.api import transform, TransformContext, Input, TransformInput, Output, TransformOutput\n\n# as should the paths to your input and output Datasets (it should have created your output Dataset based on the path and name you gave earlier)\n@transform(\n    hes=Input(\"ri.foundry.main.dataset.6f1e7e55-a2bc-44b1-9c38-ab930178c4a4\"),\n    hes_output=Output(\"/SOLEX-a8870f/[Learn] Palantir Training/warren.davies/ssp/data/hes_output\")\n)\ndef compute(\n    ctx: TransformContext, \n    hes: TransformInput, \n    hes_output: TransformOutput\n):\n    # Convert the hes TransformInput instance into a PySpark DataFrame\n    df_input = hes.dataframe()\n\n    # Our PySpark code goes here\n    df_output = df_input.limit(1)\n\n    # Pass the result of our DataProcessing to the write_dataframe() method of our TransformOutput instance\n    hes_output.write_dataframe(df_output)\n</code></pre>"},{"location":"#transforms-running-them","title":"Transforms - Running them","text":"<ul> <li>First you'll want to use the \"Preview\" button on the menu bar near the top</li> </ul> <ul> <li> <p>Through the menu at the bottom you can see checks on your code, test outputs, and outputs from previews and builds</p> </li> <li> <p>If it looks OK, then you can click \"Build\" at the top to actually save the output data to the Dataset</p> </li> </ul> <p></p>"},{"location":"#seeing-the-results","title":"Seeing the results","text":"<ul> <li> <p>Now go to the Dataset</p> </li> <li> <p>You should see your data here</p> </li> <li> <p>But note that your data branches too!</p> </li> </ul>"}]}